{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project\n",
    "# Blog Content Characterization (Morality, Emotion Analysis, Topic Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abstract\n",
    "===============\n",
    "Blogs have become crucial to our daily lives, whether it be reading newsletters, and journals, documenting our stories, or following others' stories. \n",
    "Blogs are one of the important advancements of web2.0. Blog extends to other social media like Twitter and Facebook posts. <br>As important as blogs are, their content also plays a crucial role in the daily influence of their audience and writer. \n",
    "Audiences get influenced while writers become influential.<br> Characterizing blog content is analyzing and being able to label the blog by using various social metrics that are backed by a machine learning approach. With this in mind, this work will endeavor to apply various available machine learning approaches and libraries in characterizing blogs and their content helping users understand the influence of the blog or the author and attributing the written label to the blog to help users decode the underlying message of the author that the audience may not be able to infer naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Many audiences read blogs and news articles online, and users usually bookmark their preferred blogs and subscribe to RSS (Really Simple Syndication) feeds from these blogs. <br>\n",
    "Users have a limited understanding of the fact that they are being influenced by the author while some authors have a limited understanding of how much their influence is growing. <br>\n",
    "Usually, some of these blogs may post information that may contain information or words that may help classify them rather than rely on the tags that the author may have given the website when creating the website. <br>\n",
    "\n",
    "The topic analysis is used to figure out a text's topic structure, which is a picture of what topics are in a text and how they change over time. <br>The topic analysis consists of two main tasks: topic identification and text segmentation (based on topic changes). \n",
    "\n",
    "Emotions can be expressed verbally through emotional vocabulary or through nonverbal cues like intonation of voice, facial expressions, and gestures, all of which play an important role in human communication. <br>Most human-computer interaction (HCI) systems lack emotional intelligence and are incapable of interpreting emotions. For blog information retrieval, it is essential to characterize blog content using relevant, dependable, and distinguishing tags.\n",
    "\n",
    "Although some authors traditionally set out to influence their audience, the majority of blog authors are usually of the opinion that they are expressing their point of view and things that they are passionate about.<br> Hence, being able to run some social analysis and using machine learning to classify these blogs will help the readers immensely to understand the author while the author will understand their content and the reason why their following is either decreasing or reducing depending on the preference of the author. <br>This is why we have introduced combined social analysis as a way of characterizing the collected blog data for this study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "To carry out this study, we will briefly describe each method and tool we will leverage on. \n",
    "First, we will collect blog data for about a two-year period by crawling blogs of interest. We will not use a keyword in collecting this data as we intend to use our results to describe or label these blogs by using results generated from morality, topic analysis, and emotion assessments. This will then allow us to use a classification model to classify these blogs.\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The dataset to be used for this study was crawled using a crawler tool specifically focusing on indo-pacific blogs i.e blogs that discuss key issues related to the indo-pacific region, the collected blog is then stored in a CSV file and uploaded to the GitHub page link below. The repository will also house subsequent project source code implementation. \n",
    "\n",
    "https://github.com/nakinnubis/inpacblogdata\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### Topic Analysis\n",
    "\n",
    "We will be using Latent Semantic Analysis (LSA -NLP) for topic analysis. This approach supports singular value decomposition by keeping documents and words in a semantic space for classification hence it fits into our goal of characterizing blog post content and information.\n",
    "\n",
    "\n",
    "#### Morality Analysis\n",
    "\n",
    "For morality assessment, we will use the moral foundation theory along with a probabilistic inference to identify the changes. Using the MFT algorithm for moral quantification, this NLP approach will allow us to classify each blog post according to the appropriate moral scores.\n",
    "\n",
    "#### Emotion Assessment\n",
    "\n",
    "To predict emotions, we will be using Bidirectional LSTM with a CNN  and use Plutchikâ€™s Wheel of Emotions to represent human emotions which will form the labels for each corpus of the document. We intend to have labels like joy, anger, sadness, and fear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Discussion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing into the notebook various python library for the study.\n",
    "\n",
    "Step 1.\n",
    "Import nbformat for notebook formatting\n",
    "Step 2\n",
    "Import word cloud which will be use in visualizing the top word from the LDA results\n",
    "\n",
    "Step 3.\n",
    "\n",
    "Import python lda visualization librarry this allow in visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbformat\n",
    "# Import the wordcloud library\n",
    "%pip install wordcloud\n",
    "%pip install pyldavis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4\n",
    "Import pandas library for data massaging and manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5\n",
    "Import the collected blog data which contains the blog post text and their title.\n",
    "This is done using the pandas read_csv which reads csv from the specified path\n",
    "The pandas data frame is assigned to a blogpost dataframe and the pandas head method is called on it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data into blogPosts\n",
    "blogPosts = pd.read_csv('Indo-pacific-blog-data.csv')\n",
    "\n",
    "# Print head\n",
    "# This shows us the content of the crawled blog data for analysis purpose\n",
    "blogPosts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6\n",
    "Dropping all columns that will not be useful for this study, name categories, comments_url\n",
    "The remaining preferred column is then saved back into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns that are not useful for this study examples of removed columns are 'categories', 'comments_url'\n",
    "blogPosts = blogPosts.drop(columns=['categories', 'comments_url'], axis=1)\n",
    "\n",
    "# Print out the first rows of blogPosts with new updated dataframe excluding 'categories', 'comments_url'\n",
    "blogPosts.head()\n",
    "blogPosts.to_csv(\"./results/lda/Indo-pacific-blog-post.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7\n",
    "Import regular expression library for removing unwanted text character from the post\n",
    "this is then stored into new column called blog_post_processed which contains the post text itself without the wrong characters\n",
    "\n",
    "Step 8\n",
    "\n",
    "All the blog_post_processed text are converted into lowercase text or characters\n",
    "\n",
    "Step 9\n",
    "\n",
    "The preferred columns are the written to a csv file. whic is made up of the following \n",
    "columns ['blogpost_id','title','date','blogger','tags','sentiment','location','blog_post_processed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation and unwanted dataset to allow a more clean data when we start performing LDA on the dataset\n",
    "# We used the post column for this purpose and create a new column from the dataset blog_post_processed column\n",
    "blogPosts['blog_post_processed'] = \\\n",
    "blogPosts['post'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the all text to lowercase for the processed blogpost column\n",
    "blogPosts['blog_post_processed'] = \\\n",
    "blogPosts['blog_post_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the head section which represents the first few columns present in the dataset\n",
    "blogPosts['blog_post_processed'].head()\n",
    "blog_post_processed_header = ['blogpost_id','title','date','blogger','tags','sentiment','location','blog_post_processed']\n",
    "blogPosts.to_csv(\"./results/lda/Indo-pacific-processed-post.csv\", columns=blog_post_processed_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 10\n",
    "\n",
    "Import word cloud library to visualize the top words \n",
    "\n",
    "The string is then passed into the wordcloud library to enable the results to generate wordcloud for the top text that occur frequently\n",
    "\n",
    "The word cloud to_image method is then called to show the image output of the top word which can be seen in the output figure below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ','.join(list(blogPosts['blog_post_processed'].values))\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, width=800, contour_width=2, contour_color='steelblue', height=800)\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 11\n",
    "Importing all required library for LDA and lanuage tool kits for computing nlp values\n",
    "\n",
    "Download stop words using the nltk. this is used to remove the unwanted stopwords from the texts\n",
    "\n",
    "Since the blog post data of study is written in english, the preferred stopwords are english stopwords\n",
    "\n",
    "The next thing is removing the stopwords\n",
    "\n",
    "The processed text is then stored back into a data frame and saved into a csv \n",
    "\n",
    "The next thing is printing out the words computed from the steps above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "data = blogPosts['blog_post_processed'].values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "# data_words\n",
    "print(data_words[:1][0][:len(data_words)-1])\n",
    "words = pd.DataFrame(data_words[:1][0][:len(data_words)-1])\n",
    "words.to_csv(\"results/lda/Indo-pacific-processed-words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 12\n",
    "Next we create the dictionary of LDA words and generate the term frequency \n",
    "This is used to generate a matrix the processed corpus term matrix is stored as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:len(corpus)-1])\n",
    "corpus_terms_m = pd.DataFrame(corpus[:1][0][:len(corpus)-1])\n",
    "corpus_terms_m.to_csv(\"results/lda/Indo-pacific-processed-corpus_terms_m.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 13\n",
    "\n",
    "We specify the number of topics of study  or interest. we set the number as 10  but can be either reduced or increase\n",
    "\n",
    "We build the LDA Model  using the gensim lda library\n",
    "\n",
    "We then print the keywords in the ten topics specified and also store the lda into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "doc_ld_df = pd.DataFrame(doc_lda)\n",
    "doc_ld_df.to_csv(\"results/lda/Indo-pacific-processed-doc_lda.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 14\n",
    "We generate a html visualizatiuon of the lda results using the same LDA libary .\n",
    "This makes it easy to understand how the lda results. Also we can increase or reduce the threshold of the lda to see the worlds or terms \n",
    "that exist across.\n",
    "\n",
    "The html output is also stored in an html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/lda/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/lda/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morality Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 15\n",
    "\n",
    "To further characterized the data, we apply morality foundation computation this will enable us to generate more categories or label for the data. \n",
    "\n",
    "First step is to install the seaborn library\n",
    "\n",
    "Import the pandas and other library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 16\n",
    "we create a input template to parse the data, then read it into a dataframe using pandas\n",
    "We then vusualize the head or top section of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_input = pd.read_csv('data/Indo-pacific-blog-data_morality.csv', header=None)\n",
    "template_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 17\n",
    "we use the emfdscore library to comopute the morality score for the data\n",
    "We specify the type of morality model to be used in DICT_TYPE to be the original Moral Foundations Dictionary\n",
    "We specify the scoring method to be bag of words and the output path to store the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emfdscore.scoring import score_docs \n",
    "\n",
    "num_docs = len(template_input)\n",
    "\n",
    "DICT_TYPE = 'mfd'\n",
    "PROB_MAP = ''\n",
    "SCORE_METHOD = 'bow'\n",
    "OUT_METRICS = ''\n",
    "OUT_CSV_PATH = 'mfd.csv'\n",
    "\n",
    "df = score_docs(template_input,DICT_TYPE,PROB_MAP,SCORE_METHOD,OUT_METRICS,num_docs)\n",
    "df.to_csv(OUT_CSV_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 18\n",
    "We use the pandas library to read the results of the mdf output\n",
    "We then visualize the top section of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect output \n",
    "mfd = pd.read_csv('mfd.csv')\n",
    "mfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 19\n",
    "To label the data, we select the top score or the value with the highest score to label the column for morality vice and virtue\n",
    "the value of the vice and virtue will then be use to characterized the data. the method is to help pandas in filling the column with the updated label score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_morality_data(rowData):\n",
    "    vice = {\n",
    "        'care': rowData[\"care.vice\"],\n",
    "        'fairnaess': rowData[\"fairness.vice\"],\n",
    "        'loyalty':rowData[\"loyalty.vice\"],\n",
    "        'authority':rowData[\"authority.vice\"],\n",
    "        'sanctity': rowData[\"sanctity.vice\"]\n",
    "    }\n",
    "    virtue = {\n",
    "        'care': rowData[\"care.virtue\"],\n",
    "        'fairnaess': rowData[\"fairness.virtue\"],\n",
    "        'loyalty':rowData[\"loyalty.virtue\"],\n",
    "        'authority':rowData[\"authority.virtue\"],\n",
    "        'sanctity': rowData[\"sanctity.virtue\"]\n",
    "    }\n",
    "    return max(vice, key=vice.get), max(virtue, key=virtue.get)\n",
    "\n",
    "def vice_label(rowData):\n",
    "   (vice,virtue) = label_morality_data(rowData=rowData)\n",
    "   return vice\n",
    "\n",
    "def virtue_label(rowData):\n",
    "   (vice,virtue) = label_morality_data(rowData=rowData)\n",
    "   return virtue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 20\n",
    "we filled the mfd dataframe and apply the existing method we created above to label the new column using the results of each of vice and virtue.\n",
    "\n",
    "This gives the mfd dataframe two new column namely vice and virtue with their appropriate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# care.vice\tfairness.vice\tloyalty.vice\tauthority.vice\tsanctity.vice\n",
    "mfd['vice'] = mfd.apply(lambda rowData: vice_label(rowData), axis=1)\n",
    "mfd['virtue'] = mfd.apply(lambda rowData: virtue_label(rowData), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 21\n",
    "We visualize the head of the dataframe to show the top ten data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 22\n",
    "To label the data with the score of vice and virtue, we select the top score or the value with the highest score to label the column for morality vice and virtue\n",
    "the value of the vice and virtue will then be use to characterized the data. the method is to help pandas in filling the column with the updated  score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_moraltiy_score(rowData):\n",
    "    vice = {\n",
    "        'care': rowData[\"care.vice\"],\n",
    "        'fairnaess': rowData[\"fairness.vice\"],\n",
    "        'loyalty':rowData[\"loyalty.vice\"],\n",
    "        'authority':rowData[\"authority.vice\"],\n",
    "        'sanctity': rowData[\"sanctity.vice\"]\n",
    "    }\n",
    "    virtue = {\n",
    "        'care': rowData[\"care.virtue\"],\n",
    "        'fairnaess': rowData[\"fairness.virtue\"],\n",
    "        'loyalty':rowData[\"loyalty.virtue\"],\n",
    "        'authority':rowData[\"authority.virtue\"],\n",
    "        'sanctity': rowData[\"sanctity.virtue\"]\n",
    "    }\n",
    "    vice_key = max(vice, key=vice.get)\n",
    "    virtue_key = max(virtue, key=virtue.get)\n",
    "    return vice[vice_key],virtue[virtue_key]\n",
    "def vice_score(rowData):\n",
    "   (vice,virtue) = label_moraltiy_score(rowData=rowData)\n",
    "   return vice\n",
    "\n",
    "def virtue_score(rowData):\n",
    "   (vice,virtue) = label_moraltiy_score(rowData=rowData)\n",
    "   return virtue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 23\n",
    "we filled the mfd dataframe and apply the existing method we created above to label the new column using the results of each of vice and virtue.\n",
    "\n",
    "This gives the mfd dataframe two new column namely vice_score and irtue_score with their appropriate label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfd['vice_score'] = mfd.apply(lambda rowData: vice_score(rowData), axis=1)\n",
    "mfd['virtue_score'] = mfd.apply(lambda rowData: virtue_score(rowData), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 24\n",
    "We visualize the head of the dataframe to show the top ten data rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 25\n",
    "Import various library for computing emotion analysis.\n",
    "import tensorflow\n",
    "import keras\n",
    "run pip install to install the emonet nlp library this library will be used in computing the emotion scores that is going to be used in labeling the blog data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "%pip install keras\n",
    "%pip3 install torch torchvision torchaudio\n",
    "\n",
    "%pip install git+https://github.com/UBC-NLP/EmoNet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 26\n",
    "import the emonet module from the emonet library installed\n",
    "import pandas for manipulating dataframe object\n",
    "We then create an instance of the Emonet library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emonet import EmoNet\n",
    "import pandas as pd \n",
    "em = EmoNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 27\n",
    "We read the path where the data for the blogpost data which is already labeled with other computation performed previously\n",
    "this is stored in blogPosts dataframe. We create a method called predict_labels which will be use manipulating the emotion data so we can label each row of the blogpost data with emotion scores and emotion desciption accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blogPosts = pd.read_csv('results/lda/Indo-pacific-processed-post.csv')\n",
    "processedBlogPost =  blogPosts.blog_post_processed\n",
    "def predict_labels(post):\n",
    "    predictions = em.predict(post)\n",
    "    predictions = predictions[0]\n",
    "    (label, score) = predictions \n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 28\n",
    "The predict scores method used to return the score of the generated emotion which will be used to update the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_scores(post):\n",
    "    predictions = em.predict(post)\n",
    "    predictions = predictions[0]\n",
    "    (label, score) = predictions \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 29\n",
    "we generate the emotion label and the emotion score fields and store the result in the updated blog post tables by calling the predict scores method that we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogPosts['emotion'] = blogPosts.apply(lambda row: predict_labels(row['blog_post_processed']), axis=1)\n",
    "blogPosts['emotions_score'] = blogPosts.apply(lambda row: predict_scores(row['blog_post_processed']), axis=1)\n",
    "blogPosts_result =pd.concat([blogPosts, mfd], axis=1)\n",
    "# blogPosts_result = pd.merge(blogPosts,mfd, how='outer', on =)\n",
    "blogPosts_result=blogPosts_result.drop(columns=['Unnamed: 0'], axis=1)\n",
    "blogPosts_result.to_csv('results/emos/Indo-pacific-processed-post-emotions.csv')\n",
    "blogPosts_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Text-Classification using Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 30\n",
    "This steps involves using logistics regression to characterize the blog data and also the generate the confusion matrix for the result.\n",
    "We did three difference characterization for emotion, vice and virtue which is computed from  morality using the moral foundation frame.\n",
    "\n",
    "We used the following libraries for this purpose numpy, pandas, sklearn and nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # \n",
    "import pandas as pd # \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 31\n",
    "\n",
    "This steps involves importing all the required library for plotting and visualizing the data, vectorization and term frequency library\n",
    "We also import regex and downloaded the appropriate stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sqlite3 import Error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sqlite3\n",
    "import pickle\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 32\n",
    "We read the processed blog post data which has been labelled accordingly with the emotion analysis data and morality data, we dropped the columns that are not needed and display the top ten dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('results/emos/Indo-pacific-processed-post-emotions.csv')\n",
    "dataset=dataset.drop(columns=['Unnamed: 0'], axis=1)\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 33\n",
    "We group the dataset and plot using the enmoscore column to group the data\n",
    "\n",
    "This is then plot using maplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.groupby('emotion').emotions_score.plot.bar(ylim=0)\n",
    "plt.figure(figsize=(10, 10))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 34\n",
    "This step involves downloading the stopwords and we choose english since majority of the dataset is english\n",
    "We then apply regex to remove characters that are not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "words = stopwords.words(\"english\")\n",
    "dataset['cleaned'] = dataset['blog_post_processed'].apply(lambda x: \" \".join([stemmer.stem(i) for i in re.sub(\"[^a-zA-Z]\", \" \", x).split() if i not in words]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 35\n",
    "shows the dataset top ten values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 36\n",
    "This steps involves perform the vectorization by using the term frequence and ngram  before performiong vectorization and transformation of the data this also generates a matrix from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df= 3, stop_words=\"english\", sublinear_tf=True, norm='l2', ngram_range=(1, 2))\n",
    "final_features = vectorizer.fit_transform(dataset['cleaned']).toarray()\n",
    "final_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 37\n",
    "We import the logistics regression library from sklearn\n",
    "and also used the confusion maxtrix from sklearn to generate the confusion matrix\n",
    "after importing all the preferred library we define a method characterized_by_selected_fields.\n",
    "characterized_by_selected_fields takes the dataset and the columns representing the x, y column of interest.\n",
    "We run the train by spliting the data and specifying the test size to be 25% of our input data.\n",
    "\n",
    "We also used the library to fit the data.\n",
    "\n",
    "We then generate the classfication and confusion matrix reports of the dataset for the specified label.  \n",
    "\n",
    "We also used the matplot to plot the data data\n",
    "\n",
    "This is used to generate the blog text classification using the label of the vice which is generated from the morality data\n",
    "by calling the characterized_by_selected_fields method and passing the dataset and the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def characterized_by_selected_fields(dataset, x_name, y_name):\n",
    "    X = dataset[x_name]\n",
    "    Y = dataset[y_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25)\n",
    "\n",
    "    pipeline = Pipeline([('vect', vectorizer),\n",
    "                        ('chi',  SelectKBest(chi2, k=1200)),\n",
    "                        ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    with open('LogisticRegression.pickle', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    ytest = np.array(y_test)\n",
    "\n",
    "    # confusion matrix and classification report(precision, recall, F1-score)\n",
    "    print(classification_report(ytest, model.predict(X_test)))\n",
    "    print(confusion_matrix(ytest, model.predict(X_test)))\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    plot_confusion_matrix(model, X_test, ytest,ax=ax)  \n",
    "    plt.show()\n",
    "    \n",
    "characterized_by_selected_fields(dataset,'cleaned','emotion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 38\n",
    "This is used to generate the blog text classification using the label of the vice which is generated from the morality data\n",
    "by calling the characterized_by_selected_fields method and passing the dataset and the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterized_by_selected_fields(dataset,'cleaned','vice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 39\n",
    "This is used to generate the blog text classification using the label of the virtue which is generated from the morality data\n",
    "by calling the characterized_by_selected_fields method and passing the dataset and the columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterized_by_selected_fields(dataset,'cleaned','virtue')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
